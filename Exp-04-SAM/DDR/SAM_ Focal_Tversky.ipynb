{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ab8cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from segment_anything import sam_model_registry\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf2df3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42) -> None:\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)           \n",
    "    random.seed(seed)                                 \n",
    "    np.random.seed(seed)                              \n",
    "    torch.manual_seed(seed)                           \n",
    "    torch.cuda.manual_seed(seed)                       \n",
    "    torch.cuda.manual_seed_all(seed)                   \n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    torch.backends.cudnn.deterministic = True         \n",
    "    torch.backends.cudnn.benchmark = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7dffb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuration\n",
    "\n",
    "\n",
    "dataset_path = \"./final_dataset\"               # root containing train/ & test/\n",
    "checkpoint_path = \"../sam_vit_h_4b8939.pth\"     # SAM ViT‑H checkpoint\n",
    "model_type = \"vit_h\"                           # sam encoder type\n",
    "output_dir = \"./predictions\"                   # where to save models & figs\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dde35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset\n",
    "\n",
    "class RetinalHemorrhageDataset(Dataset):\n",
    "    \"\"\"Dataset loading RGB fundus images and binary masks.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir: str, split: str = \"train\", transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.images_dir = os.path.join(root_dir, split, \"images\")\n",
    "        self.masks_dir = os.path.join(root_dir, split, \"masks\")\n",
    "        self.image_files = sorted(glob.glob(os.path.join(self.images_dir, \"*.*\")))\n",
    "        self.mask_files = sorted(glob.glob(os.path.join(self.masks_dir, \"*.*\")))\n",
    "\n",
    "        assert len(self.image_files) == len(self.mask_files), (\n",
    "            f\"Number of images ({len(self.image_files)}) and masks \"\n",
    "            f\"({len(self.mask_files)}) don't match!\"\n",
    "        )\n",
    "        print(f\"Found {len(self.image_files)} samples in '{split}' split\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Image\n",
    "        img_path = self.image_files[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Mask\n",
    "        msk_path = self.mask_files[idx]\n",
    "        msk = cv2.imread(msk_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if msk is None:\n",
    "            raise ValueError(f\"Failed to load mask: {msk_path}\")\n",
    "        msk = (msk > 0).astype(np.float32)\n",
    "\n",
    "        # To tensor\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0  # C,H,W\n",
    "        msk = torch.from_numpy(msk).unsqueeze(0).float()              # 1,H,W\n",
    "        return {\"image\": img, \"mask\": msk, \"filename\": os.path.basename(img_path)}\n",
    "\n",
    "\n",
    "# Loss\n",
    "\n",
    "class FocalTverskyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.3, beta=0.7, gamma=0.75, eps=1e-6):\n",
    "        super(FocalTverskyLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        TP = (inputs * targets).sum()\n",
    "        FP = ((1 - targets) * inputs).sum()\n",
    "        FN = (targets * (1 - inputs)).sum()\n",
    "\n",
    "        tversky = (TP + self.eps) / (TP + self.alpha * FP + self.beta * FN + self.eps)\n",
    "        focal_tversky = (1 - tversky) ** self.gamma\n",
    "\n",
    "        return focal_tversky\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959a14c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# U‑Net style decoder blocks\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNetDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch: int = 256):\n",
    "        super().__init__()\n",
    "        self.block1 = DoubleConv(in_ch, 256)\n",
    "        self.up1 = UpBlock(256, 128)\n",
    "        self.up2 = UpBlock(128, 64)\n",
    "        self.up3 = UpBlock(64, 32)\n",
    "        self.up4 = UpBlock(32, 16)\n",
    "        self.final = nn.Conv2d(16, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.up3(x)\n",
    "        x = self.up4(x)\n",
    "        return self.final(x)\n",
    "\n",
    "\n",
    "# Fine‑tuner combining SAM encoder + U‑Net decoder\n",
    "\n",
    "\n",
    "class SAMFineTuner(nn.Module):\n",
    "    def __init__(self, checkpoint_path: str, model_type: str):\n",
    "        super().__init__()\n",
    "        print(f\"Loading SAM encoder '{model_type}' …\")\n",
    "        self.sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
    "        self.sam.to(device)\n",
    "        print(\"SAM encoder loaded.\")\n",
    "\n",
    "        # Freeze encoder parameters\n",
    "        for p in self.sam.image_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = UNetDecoder(in_ch=256)\n",
    "\n",
    "        # Pre‑processing helper\n",
    "        self.transform = ResizeLongestSide(self.sam.image_encoder.img_size)\n",
    "        self.pixel_mean = torch.tensor([123.675, 116.28, 103.53]).view(3, 1, 1)\n",
    "        self.pixel_std = torch.tensor([58.395, 57.12, 57.375]).view(3, 1, 1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def preprocess_single(self, img_tensor: torch.Tensor) -> torch.Tensor:\n",
    "       \n",
    "        img_np = (img_tensor.cpu().numpy().transpose(1, 2, 0) * 255.0).astype(np.float32)\n",
    "        img_np = self.transform.apply_image(img_np)\n",
    "        t = torch.from_numpy(img_np).permute(2, 0, 1).float()\n",
    "        t = (t - self.pixel_mean) / self.pixel_std\n",
    "        return t\n",
    "\n",
    "    def forward(self, imgs: torch.Tensor):  # imgs: B,C,H,W in [0,1]\n",
    "        B, _, H, W = imgs.shape\n",
    "        processed = torch.stack([self.preprocess_single(im) for im in imgs]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            emb = self.sam.image_encoder(processed)  # B,256,h,w\n",
    "\n",
    "        dec_out = self.decoder(emb)                 # B,1,h',w'\n",
    "        dec_out = F.interpolate(dec_out, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
    "        return torch.sigmoid(dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Metrics\n",
    "\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray, thr: float = 0.5):\n",
    "    y_pred_bin = (y_pred > thr).astype(np.uint8)\n",
    "    y_true = y_true.astype(np.uint8)\n",
    "    tp = np.sum((y_pred_bin == 1) & (y_true == 1))\n",
    "    fp = np.sum((y_pred_bin == 1) & (y_true == 0))\n",
    "    fn = np.sum((y_pred_bin == 0) & (y_true == 1))\n",
    "    tn = np.sum((y_pred_bin == 0) & (y_true == 0))\n",
    "\n",
    "    acc = (tp + tn) / (tp + fp + fn + tn + 1e-6)\n",
    "    precision = tp / (tp + fp + 1e-6)\n",
    "    recall = tp / (tp + fn + 1e-6)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "    iou = tp / (tp + fp + fn + 1e-6)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1_score\": f1, \"jaccard\": iou}\n",
    "\n",
    "# Train / Eval loops\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
    "    for batch in pbar:\n",
    "        imgs = batch[\"image\"].to(device)\n",
    "        msks = batch[\"mask\"].to(device)\n",
    "        preds = model(imgs)\n",
    "        loss = criterion(preds, msks)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion, save_preds: bool = False):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    stats = {k: [] for k in [\"accuracy\", \"precision\", \"recall\", \"f1_score\", \"jaccard\"]}\n",
    "    pbar = tqdm(loader, desc=\"Eval\")\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            imgs = batch[\"image\"].to(device)\n",
    "            msks = batch[\"mask\"].to(device)\n",
    "            fnames = batch[\"filename\"]\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, msks)\n",
    "            val_loss += loss.item()\n",
    "            preds_np = preds.cpu().numpy()\n",
    "            msks_np = msks.cpu().numpy()\n",
    "            for i in range(len(imgs)):\n",
    "                metrics = compute_metrics(msks_np[i, 0], preds_np[i, 0])\n",
    "                for k in stats:\n",
    "                    stats[k].append(metrics[k])\n",
    "                if save_preds:\n",
    "                    save_visualization(imgs[i], msks_np[i, 0], preds_np[i, 0], metrics, fnames[i])\n",
    "    avg_stats = {k: float(np.mean(v)) for k, v in stats.items()}\n",
    "    return val_loss / len(loader), avg_stats\n",
    "\n",
    "\n",
    "# Helper to save side‑by‑side visualizations\n",
    "\n",
    "\n",
    "def save_visualization(img_t, true_msk, pred_msk, metrics, fname):\n",
    "    img = (img_t.cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)\n",
    "    pred = (pred_msk > 0.5).astype(np.uint8) * 255\n",
    "    true = (true_msk > 0.5).astype(np.uint8) * 255\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(\"Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[1].imshow(true, cmap=\"gray\")\n",
    "    axes[1].set_title(\"Ground Truth\")\n",
    "    axes[1].axis(\"off\")\n",
    "    axes[2].imshow(pred, cmap=\"gray\")\n",
    "    axes[2].set_title(\"Prediction\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    txt = (\n",
    "        f\"IoU={metrics['jaccard']:.3f} | F1={metrics['f1_score']:.3f} | \"\n",
    "        f\"Prec={metrics['precision']:.3f} | Rec={metrics['recall']:.3f}\"\n",
    "    )\n",
    "    \n",
    "    # Add more space at the bottom for the text\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    fig.text(0.5, 0.05, txt, ha=\"center\", fontsize=10)\n",
    "\n",
    "    out_path = os.path.join(output_dir, f\"{os.path.splitext(fname)[0]}_viz.png\")\n",
    "    plt.savefig(out_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5996fd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3192 samples in 'train' split\n",
      "Found 225 samples in 'test' split\n",
      "Loading SAM encoder 'vit_h' …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/HS401/in00199/.local/lib/python3.11/site-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM encoder loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/HS401/in00199/.local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# Data\n",
    "train_ds = RetinalHemorrhageDataset(dataset_path, split=\"train\")\n",
    "full_test_ds = RetinalHemorrhageDataset(dataset_path, split=\"test\")\n",
    "\n",
    "# Split test dataset into fixed 50% validation and 50% test\n",
    "val_size = len(full_test_ds) // 2\n",
    "test_size = len(full_test_ds) - val_size\n",
    "val_ds, test_ds = torch.utils.data.random_split(full_test_ds, [val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Model / loss / optim\n",
    "model = SAMFineTuner(checkpoint_path, model_type).to(device)\n",
    "criterion = FocalTverskyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.1,\n",
    "    patience=4,\n",
    "    threshold=1e-4,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/3192 [00:00<?, ?it/s]/user/HS401/in00199/.local/lib/python3.11/site-packages/segment_anything/modeling/image_encoder.py:231: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at ../aten/src/ATen/Context.cpp:208.)\n",
      "  attn = (q * self.scale) @ k.transpose(-2, -1)\n",
      "/user/HS401/in00199/.local/lib/python3.11/site-packages/torch/functional.py:402: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at ../aten/src/ATen/Context.cpp:208.)\n",
      "  return _VF.einsum(equation, operands)  # type: ignore[attr-defined]\n",
      "/user/HS401/in00199/.local/lib/python3.11/site-packages/segment_anything/modeling/image_encoder.py:237: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at ../aten/src/ATen/Context.cpp:208.)\n",
      "  x = (attn @ v).view(B, self.num_heads, H, W, -1).permute(0, 2, 3, 1, 4).reshape(B, H, W, -1)\n",
      "Epoch 1: 100%|██████████| 3192/3192 [1:09:03<00:00,  1.30s/it, loss=1.0000]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Loss: 0.9833 | Val Loss: 0.9591 | IoU: 0.0363 | F1: 0.0655 | Precision: 0.0410 | Recall: 0.4877 | Accuracy: 0.9112\n",
      "New best model saved (F1: 0.0655)\n",
      "Current lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3192/3192 [1:08:05<00:00,  1.28s/it, loss=1.0000]\n",
      "Eval: 100%|██████████| 112/112 [01:55<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 | Train Loss: 0.9644 | Val Loss: 0.9077 | IoU: 0.1110 | F1: 0.1842 | Precision: 0.1709 | Recall: 0.3340 | Accuracy: 0.9850\n",
      "New best model saved (F1: 0.1842)\n",
      "Current lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3192/3192 [1:08:09<00:00,  1.28s/it, loss=1.0000]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 | Train Loss: 0.8952 | Val Loss: 0.8307 | IoU: 0.1334 | F1: 0.2136 | Precision: 0.2274 | Recall: 0.2851 | Accuracy: 0.9885\n",
      "New best model saved (F1: 0.2136)\n",
      "Current lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 3192/3192 [1:08:20<00:00,  1.28s/it, loss=0.9959]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 | Train Loss: 0.8260 | Val Loss: 0.8230 | IoU: 0.1533 | F1: 0.2374 | Precision: 0.3891 | Recall: 0.2169 | Accuracy: 0.9914\n",
      "New best model saved (F1: 0.2374)\n",
      "Current lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 3192/3192 [1:08:24<00:00,  1.29s/it, loss=0.7270]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 | Train Loss: 0.7989 | Val Loss: 0.8231 | IoU: 0.1459 | F1: 0.2260 | Precision: 0.3303 | Recall: 0.2283 | Accuracy: 0.9910\n",
      "No improvement in F1 for 1 epoch(s)\n",
      "Current lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 3192/3192 [1:08:28<00:00,  1.29s/it, loss=0.9626]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 | Train Loss: 0.7857 | Val Loss: 0.8167 | IoU: 0.1584 | F1: 0.2445 | Precision: 0.4052 | Recall: 0.2177 | Accuracy: 0.9916\n",
      "New best model saved (F1: 0.2445)\n",
      "Current lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 3192/3192 [1:08:28<00:00,  1.29s/it, loss=0.5423]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 | Train Loss: 0.7787 | Val Loss: 0.8099 | IoU: 0.1609 | F1: 0.2484 | Precision: 0.3893 | Recall: 0.2326 | Accuracy: 0.9915\n",
      "New best model saved (F1: 0.2484)\n",
      "Current lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 3192/3192 [1:08:30<00:00,  1.29s/it, loss=1.0000]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 | Train Loss: 0.7703 | Val Loss: 0.8032 | IoU: 0.1618 | F1: 0.2503 | Precision: 0.3604 | Recall: 0.2499 | Accuracy: 0.9912\n",
      "New best model saved (F1: 0.2503)\n",
      "Current lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 3192/3192 [1:08:26<00:00,  1.29s/it, loss=1.0000]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 | Train Loss: 0.7614 | Val Loss: 0.8073 | IoU: 0.1653 | F1: 0.2537 | Precision: 0.4099 | Recall: 0.2318 | Accuracy: 0.9916\n",
      "New best model saved (F1: 0.2537)\n",
      "Current lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 3192/3192 [1:08:09<00:00,  1.28s/it, loss=1.0000]\n",
      "Eval: 100%|██████████| 112/112 [01:55<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Train Loss: 0.7540 | Val Loss: 0.8020 | IoU: 0.1578 | F1: 0.2466 | Precision: 0.3311 | Recall: 0.2702 | Accuracy: 0.9909\n",
      "No improvement in F1 for 1 epoch(s)\n",
      "Current lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 3192/3192 [1:08:14<00:00,  1.28s/it, loss=1.0000]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 | Train Loss: 0.7514 | Val Loss: 0.7888 | IoU: 0.1669 | F1: 0.2588 | Precision: 0.3254 | Recall: 0.2911 | Accuracy: 0.9905\n",
      "New best model saved (F1: 0.2588)\n",
      "Current lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 3192/3192 [1:08:22<00:00,  1.29s/it, loss=0.4927]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 | Train Loss: 0.7534 | Val Loss: 0.8022 | IoU: 0.1693 | F1: 0.2581 | Precision: 0.4298 | Recall: 0.2371 | Accuracy: 0.9916\n",
      "No improvement in F1 for 1 epoch(s)\n",
      "Current lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 3192/3192 [1:08:14<00:00,  1.28s/it, loss=0.6677]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 | Train Loss: 0.7446 | Val Loss: 0.8073 | IoU: 0.1707 | F1: 0.2573 | Precision: 0.4603 | Recall: 0.2210 | Accuracy: 0.9917\n",
      "No improvement in F1 for 2 epoch(s)\n",
      "Current lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 3192/3192 [1:08:07<00:00,  1.28s/it, loss=0.4698]\n",
      "Eval: 100%|██████████| 112/112 [01:55<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 | Train Loss: 0.7396 | Val Loss: 0.7994 | IoU: 0.1726 | F1: 0.2621 | Precision: 0.4510 | Recall: 0.2411 | Accuracy: 0.9917\n",
      "New best model saved (F1: 0.2621)\n",
      "Current lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 3192/3192 [1:08:09<00:00,  1.28s/it, loss=1.0000]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 | Train Loss: 0.7360 | Val Loss: 0.7945 | IoU: 0.1763 | F1: 0.2673 | Precision: 0.4384 | Recall: 0.2485 | Accuracy: 0.9917\n",
      "New best model saved (F1: 0.2673)\n",
      "Current lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 3192/3192 [1:08:12<00:00,  1.28s/it, loss=1.0000]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 | Train Loss: 0.7331 | Val Loss: 0.7947 | IoU: 0.1647 | F1: 0.2520 | Precision: 0.3072 | Recall: 0.2804 | Accuracy: 0.9902\n",
      "No improvement in F1 for 1 epoch(s)\n",
      "Current lr: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 3192/3192 [1:08:13<00:00,  1.28s/it, loss=1.0000]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 | Train Loss: 0.7110 | Val Loss: 0.7975 | IoU: 0.1753 | F1: 0.2657 | Precision: 0.4356 | Recall: 0.2428 | Accuracy: 0.9918\n",
      "No improvement in F1 for 2 epoch(s)\n",
      "Current lr: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 3192/3192 [1:08:15<00:00,  1.28s/it, loss=0.7691]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 | Train Loss: 0.7008 | Val Loss: 0.7991 | IoU: 0.1754 | F1: 0.2645 | Precision: 0.4453 | Recall: 0.2371 | Accuracy: 0.9918\n",
      "No improvement in F1 for 3 epoch(s)\n",
      "Current lr: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 3192/3192 [1:08:18<00:00,  1.28s/it, loss=1.0000]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 | Train Loss: 0.6964 | Val Loss: 0.7945 | IoU: 0.1773 | F1: 0.2682 | Precision: 0.4447 | Recall: 0.2461 | Accuracy: 0.9918\n",
      "New best model saved (F1: 0.2682)\n",
      "Current lr: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 3192/3192 [1:08:10<00:00,  1.28s/it, loss=0.4012]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 | Train Loss: 0.6924 | Val Loss: 0.7971 | IoU: 0.1757 | F1: 0.2666 | Precision: 0.4410 | Recall: 0.2419 | Accuracy: 0.9918\n",
      "No improvement in F1 for 1 epoch(s)\n",
      "Current lr: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 3192/3192 [1:08:09<00:00,  1.28s/it, loss=0.6623]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 | Train Loss: 0.6894 | Val Loss: 0.8050 | IoU: 0.1708 | F1: 0.2601 | Precision: 0.4598 | Recall: 0.2273 | Accuracy: 0.9919\n",
      "No improvement in F1 for 2 epoch(s)\n",
      "Current lr: 1.0000000000000002e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 3192/3192 [1:08:10<00:00,  1.28s/it, loss=0.3795]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 | Train Loss: 0.6861 | Val Loss: 0.7977 | IoU: 0.1754 | F1: 0.2664 | Precision: 0.4493 | Recall: 0.2391 | Accuracy: 0.9918\n",
      "No improvement in F1 for 3 epoch(s)\n",
      "Current lr: 1.0000000000000002e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 3192/3192 [1:08:09<00:00,  1.28s/it, loss=0.6156]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 | Train Loss: 0.6846 | Val Loss: 0.8000 | IoU: 0.1749 | F1: 0.2651 | Precision: 0.4592 | Recall: 0.2349 | Accuracy: 0.9919\n",
      "No improvement in F1 for 4 epoch(s)\n",
      "Current lr: 1.0000000000000002e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 3192/3192 [1:08:28<00:00,  1.29s/it, loss=0.1380]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 | Train Loss: 0.6838 | Val Loss: 0.7978 | IoU: 0.1753 | F1: 0.2661 | Precision: 0.4507 | Recall: 0.2388 | Accuracy: 0.9918\n",
      "No improvement in F1 for 5 epoch(s)\n",
      "Current lr: 1.0000000000000002e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 3192/3192 [1:08:32<00:00,  1.29s/it, loss=0.9849]\n",
      "Eval: 100%|██████████| 112/112 [01:56<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 | Train Loss: 0.6831 | Val Loss: 0.7940 | IoU: 0.1777 | F1: 0.2692 | Precision: 0.4411 | Recall: 0.2465 | Accuracy: 0.9918\n",
      "New best model saved (F1: 0.2692)\n",
      "Current lr: 1.0000000000000002e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 3192/3192 [1:08:32<00:00,  1.29s/it, loss=1.0000]\n",
      "Eval: 100%|██████████| 112/112 [01:57<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 | Train Loss: 0.6824 | Val Loss: 0.7994 | IoU: 0.1754 | F1: 0.2654 | Precision: 0.4672 | Recall: 0.2357 | Accuracy: 0.9919\n",
      "No improvement in F1 for 1 epoch(s)\n",
      "Current lr: 1.0000000000000002e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27:  46%|████▌     | 1474/3192 [31:42<36:57,  1.29s/it, loss=0.8312] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m no_improve_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, criterion, optimizer, epoch)\n\u001b[1;32m      9\u001b[0m     val_loss, metrics \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion)\n\u001b[1;32m     10\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(val_loss)\n",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     29\u001b[0m imgs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m msks \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 31\u001b[0m preds \u001b[38;5;241m=\u001b[39m model(imgs)\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(preds, msks)\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 87\u001b[0m, in \u001b[0;36mSAMFineTuner.forward\u001b[0;34m(self, imgs)\u001b[0m\n\u001b[1;32m     84\u001b[0m processed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_single(im) \u001b[38;5;28;01mfor\u001b[39;00m im \u001b[38;5;129;01min\u001b[39;00m imgs])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 87\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msam\u001b[38;5;241m.\u001b[39mimage_encoder(processed)  \u001b[38;5;66;03m# B,256,h,w\u001b[39;00m\n\u001b[1;32m     89\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(emb)                 \u001b[38;5;66;03m# B,1,h',w'\u001b[39;00m\n\u001b[1;32m     90\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(dec_out, size\u001b[38;5;241m=\u001b[39m(H, W), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/segment_anything/modeling/image_encoder.py:112\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m blk(x)\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/segment_anything/modeling/image_encoder.py:174\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m     H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    172\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 174\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(x)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/segment_anything/modeling/image_encoder.py:234\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    231\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[0;32m--> 234\u001b[0m     attn \u001b[38;5;241m=\u001b[39m add_decomposed_rel_pos(attn, q, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_pos_h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_pos_w, (H, W), (H, W))\n\u001b[1;32m    236\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    237\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/segment_anything/modeling/image_encoder.py:350\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    348\u001b[0m k_h, k_w \u001b[38;5;241m=\u001b[39m k_size\n\u001b[1;32m    349\u001b[0m Rh \u001b[38;5;241m=\u001b[39m get_rel_pos(q_h, k_h, rel_pos_h)\n\u001b[0;32m--> 350\u001b[0m Rw \u001b[38;5;241m=\u001b[39m get_rel_pos(q_w, k_w, rel_pos_w)\n\u001b[1;32m    352\u001b[0m B, _, dim \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    353\u001b[0m r_q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mreshape(B, q_h, q_w, dim)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/segment_anything/modeling/image_encoder.py:292\u001b[0m, in \u001b[0;36mget_rel_pos\u001b[0;34m(q_size, k_size, rel_pos)\u001b[0m\n\u001b[1;32m    288\u001b[0m         x \u001b[38;5;241m=\u001b[39m x[:, :H, :W, :]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m--> 292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_rel_pos\u001b[39m(q_size: \u001b[38;5;28mint\u001b[39m, k_size: \u001b[38;5;28mint\u001b[39m, rel_pos: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    293\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    Get relative positional embeddings according to the relative positions of\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m        query and key sizes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03m        Extracted positional embeddings according to relative positions.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     max_rel_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(q_size, k_size) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "num_epochs = 30\n",
    "best_f1 = -1.0\n",
    "patience = 8\n",
    "no_improve_epochs = 0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    tr_loss = train_epoch(model, train_loader, criterion, optimizer, epoch)\n",
    "    val_loss, metrics = evaluate(model, val_loader, criterion)\n",
    "    scheduler.step(val_loss)\n",
    "    lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{num_epochs} | \"\n",
    "        f\"Train Loss: {tr_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "        f\"IoU: {metrics['jaccard']:.4f} | \"\n",
    "        f\"F1: {metrics['f1_score']:.4f} | \"\n",
    "        f\"Precision: {metrics['precision']:.4f} | \"\n",
    "        f\"Recall: {metrics['recall']:.4f} | \"\n",
    "        f\"Accuracy: {metrics['accuracy']:.4f}\"\n",
    "    )\n",
    "\n",
    "    if metrics[\"f1_score\"] > best_f1:\n",
    "        best_f1 = metrics[\"f1_score\"]\n",
    "        no_improve_epochs = 0  # reset counter\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, \"best_model.pth\"))\n",
    "        print(f\"New best model saved (F1: {best_f1:.4f})\")\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f\"No improvement in F1 for {no_improve_epochs} epoch(s)\")\n",
    "\n",
    "    print(f\"Current lr: {lr_now}\")\n",
    "\n",
    "    if no_improve_epochs >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fae23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualizations for best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53679/2455891843.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(output_dir, \"best_model.pth\")))\n",
      "Eval:  13%|█▎        | 15/113 [00:18<01:55,  1.18s/it]"
     ]
    }
   ],
   "source": [
    "# Final evaluation with visualisations\n",
    "print(\"Generating visualizations for best model\")\n",
    "model.load_state_dict(torch.load(os.path.join(output_dir, \"best_model.pth\")))\n",
    "_, final_metrics = evaluate(model, test_loader, criterion, save_preds=True)\n",
    "print(\"Final Test Metrics:\")\n",
    "for k, v in final_metrics.items():\n",
    "    print(f\"  {k.capitalize()}: {v:.4f}\")\n",
    "print(f\"Predictions & visualizations saved to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
